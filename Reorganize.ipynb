{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import time\n",
    "\n",
    "### Debug only\n",
    "__file__ = r\"./Reorganize.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Entso-E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(ctry, start=None, end=None, freq=\"H\", target=\"CH\",\n",
    "                involved_countries=None, prod_gap=None, sg_data=None,\n",
    "                path_gen=None, path_imp=None, residual_global=False,\n",
    "                correct_imp=True, n_hours=2, days_around=7, is_verbose=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Main function managing the import and pre-treatment of Entso-e production and cross-border flow data.\n",
    "    \n",
    "    Patameter:\n",
    "        ctry: list of countries to include in the computing (list)\n",
    "        start: starting date (str or datetime)\n",
    "        end: ending date (str or datetime)\n",
    "        freq: time step (str, default: H)\n",
    "        target: target country (str, default: CH)\n",
    "        involved_countries: list of all countries involved, with the countries to include in the computing\n",
    "                            and their neighbours (to implement the exchanges with 'Other' countries)\n",
    "                            (list, default: None)\n",
    "        prod_gap: information about the nature of the residual (pandas DataFrame)\n",
    "        sg_data: information from Swiss Grid (pandas DataFrame, default: None)\n",
    "        path_gen: directory where Entso-e generation files are stored (str)\n",
    "        path_imp: directory containing the files for cross-border flow data (str)\n",
    "        residual_global: to consider the production residual as produced electricity that can be\n",
    "                        exchanged with neighbour countries (bool, default: False)\n",
    "        correct_imp: to replace cross-border flow of Entso-e for Swizerland with data from Swiss Grid\n",
    "                    (bool, default: False)\n",
    "        n_hours: Max number of successive missing hours to be considered as occasional event\n",
    "                (int, default: 2)\n",
    "        days_around: number of days after and before a gap to consider to create a 'typical mean day'\n",
    "                (int, default: 7)\n",
    "        is_verbose: to display information (bool, default: False)\n",
    "    \n",
    "    Return:\n",
    "        pandas DataFrame with all productions and all exchanges from all included countries.\n",
    "    \"\"\"\n",
    "    \n",
    "    t0 = time()\n",
    "    \n",
    "    ### GENERATION DATA\n",
    "    Gen = import_generation(path_gen=path_gen, ctry=ctry, start=start, end=end,\n",
    "                            is_verbose=is_verbose) # import generation data\n",
    "    Gen = adjust_generation(Gen, freq=freq, residual_global=residual_global, sg_data=sg_data,\n",
    "                            n_hours=n_hours, days_around=days_around, prod_gap=prod_gap,\n",
    "                            is_verbose=is_verbose) # adjust the generation data\n",
    "    \n",
    "    ### EXCHANGE DATA\n",
    "    Cross = import_exchanges(path_imp=path_imp, neighbourhood=involved_countries, ctry=ctry,\n",
    "                             start=start, end=end, freq=freq, is_verbose=is_verbose) # Imprt data\n",
    "    \n",
    "    # Correct the cross-border flows at Swiss border.\n",
    "    if correct_imp:\n",
    "        if is_verbose: print(\"Adapt Exchage Data of Swizerland...\")\n",
    "        Cross = adjust_exchanges(Cross=Cross,sg_data=sg_data,freq=freq) # Adjust the exchange data\n",
    "    \n",
    "    ### GATHER GENERATION AND EXCHANGE\n",
    "    electric_mix = _join_generation_exchanges(Gen=Gen, Cross=Cross, is_verbose=is_verbose)\n",
    "    \n",
    "    \n",
    "    if is_verbose: print(\"Import of data: {:.1f} sec\".format(time()-t0))\n",
    "    return electric_mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_generation(path_gen, ctry, start, end, is_verbose=False):\n",
    "    \"\"\"\n",
    "    Function to import generation data from Entso-e information source.\n",
    "    \n",
    "    Parameter:\n",
    "        path_gen: directory where Entso-e generation files are stored (str)\n",
    "        ctry: countries to incldue in the study (list)\n",
    "        start: starting date (str or datetime)\n",
    "        end: ending date (str or datetime)\n",
    "        is_verbose: to display information (bool, default: False)\n",
    "    \"\"\"\n",
    "    #######################\n",
    "    ###### Generation data\n",
    "    #######################\n",
    "\n",
    "    if is_verbose: print(\"Load generation data...\")\n",
    "    # Selecton of right files according to the choice of countres\n",
    "    files = {}\n",
    "    for c in ctry:\n",
    "        try:\n",
    "            files[c] = [f for f in os.listdir(path_gen)\n",
    "                        if ((f.split(\"_\")[5]==c) & (f.split(\"_\")[0]=='2021'))][0]\n",
    "        except Exception as e:\n",
    "            raise KeyError(f\"No generation data for {c}: {e}\")\n",
    "    \n",
    "    Gen = {} # Dict for the generation of each country\n",
    "    for c in files:# File extraction\n",
    "        # Extract the generation data file\n",
    "        Gen[c] = pd.read_csv(path_gen+files[c],sep=\";\") # Extraction\n",
    "\n",
    "        # Set time info from UTC to Central European Time\n",
    "        Gen[c].index = pd.to_datetime(Gen[c].index,yearfirst=True) # Convert index into datetime\n",
    "        \n",
    "        # Only select the required piece of information\n",
    "        Gen[c] = Gen[c].loc[start:end]\n",
    "        \n",
    "        # RESAMPLE hourly -> 15min: to be removed\n",
    "        Gen[c] = Gen[c].resample('15min').asfreq(None).interpolate('linear') / 4 # Energy, not power.\n",
    "\n",
    "        source = list(Gen[c].columns) # production plants types\n",
    "        source[source.index(\"Other \")] = \"Other fossil \" # rename one specific column (space at the end is important !)\n",
    "\n",
    "        Gen[c].columns = [s.replace(\" \",\"_\")+c for s in source] # rename columns\n",
    "        \n",
    "    return Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_generation(Gen, freq='H', residual_global=False, sg_data=None, n_hours=2, days_around=7,\n",
    "                      prod_gap=None, is_verbose=False):\n",
    "    \"\"\"Function that leads organizes the data adjustment.\n",
    "    It sorts finds and sorts missing values, fills it, resample the data and\n",
    "    add a residual as global production\n",
    "    \n",
    "    Parameter:\n",
    "        Gen: dict of dataFrames containing the generation for each country\n",
    "        freq: time step durtion (str, default: H)\n",
    "        residual_prod: whether to include the residual or not (bool, default: False)\n",
    "        sg_data: information from Swiss Grid (pandas DataFrame, default: None)\n",
    "        n_hours: Max number of successive missing hours to be considered as occasional event\n",
    "                (int, default: 2)\n",
    "        days_around: number of days after and before a gap to consider to create a 'typical mean day'\n",
    "                (int, default: 7)\n",
    "        prod_gap: information about the nature of the residual (pandas DataFrame)\n",
    "        is_verbose: bool. Whether to display information or not.\n",
    "        \n",
    "    Return\n",
    "        dict of pandas DataFrames: modified Gen dict.\n",
    "    \"\"\"\n",
    "    ### Identify missing values\n",
    "    Empty_Gen, Empty_Nuc = gather_missing_generation(Gen=Gen,\n",
    "                                                      add_on=residual_global,\n",
    "                                                      is_verbose=is_verbose)\n",
    "    ### Classify missing values as occasional or missing period\n",
    "    Empty_Unique, Empty_Period = sort_missing_generation(Empty_Gen=Empty_Gen,\n",
    "                                                          Empty_Nuc=Empty_Nuc,\n",
    "                                                          n_hours=n_hours,\n",
    "                                                          add_on=residual_global,\n",
    "                                                          is_verbose=is_verbose)\n",
    "    ### Fill missing values consequently\n",
    "    Gen = fill_missing_generation(Gen,\n",
    "                                   Empty_Unique=Empty_Unique,\n",
    "                                   Empty_Period=Empty_Period,\n",
    "                                   n_hours=n_hours,\n",
    "                                   days_around=days_around,\n",
    "                                   add_on=residual_global,\n",
    "                                   is_verbose=is_verbose)\n",
    "    \n",
    "    ### Resample data to the right frequence\n",
    "    Gen = resample_generation(Gen=Gen, freq=freq, add_on=residual_global, is_verbose=is_verbose)\n",
    "    \n",
    "    ### Includes residual production\n",
    "    if residual_global:\n",
    "        Gen = include_global_residual(Gen=Gen, freq=freq, sg_data=sg_data, prod_gap=prod_gap,\n",
    "                                       is_verbose=is_verbose)\n",
    "    \n",
    "    return Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gather missing generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_missing_generation(Gen, add_on=False, is_verbose=False):\n",
    "    \"\"\"\n",
    "    Function to find missing information in the Entso-e data.\n",
    "    It distinguishes missing values and missing nuclear generation.\n",
    "    Parameter:\n",
    "        Gen: dict of pandas DataFrame with production data for each country.\n",
    "        add_on: display flourish (bool, default: False)\n",
    "        is_verbose: to display information (bool, default: False)\n",
    "    Return:\n",
    "        2 dict of lists, one with dates of missing generations and one with dates of missing nuclear\n",
    "    \"\"\"\n",
    "    #######################\n",
    "    ### Correction of Generation data\n",
    "    #######################\n",
    "    if is_verbose: print(f\"Correction of generation data:\\n\\t1/{4+int(add_on)} - Gather missing values...\")\n",
    "    ### Data with no nuclear production (good repair for missng datas)\n",
    "    ## Exceptions to nuclear missing datas\n",
    "    NoNuc = [\"CH\"] # There is no problem with swiss -> missing values are OK !\n",
    "    for f in Gen.keys(): # for all countries\n",
    "        if Gen[f][\"Nuclear_{}\".format(f)].sum()==0: # if the sum of all nuclear production is null\n",
    "            NoNuc.append(f) # register the country as non nuclear producer\n",
    "    \n",
    "    \n",
    "    ### Dates with partial or total missing production\n",
    "    Empty_Gen = {} # total missing production\n",
    "    Empty_Nuc = {} # partial missing production\n",
    "    empty_nuc = [] # to store some missing data\n",
    "    for f in Gen.keys(): # for all considered countries\n",
    "        # Missing production\n",
    "        Empty_Gen[f] = list(Gen[f][Gen[f].sum(axis=1)==0].index) # list of dates of missing production\n",
    "        \n",
    "        # Missing nuclear\n",
    "        if f in NoNuc: # if country without nuclear\n",
    "            Empty_Nuc[f]=[] # nothing to look for\n",
    "        else:\n",
    "            Empty_Nuc[f] = []\n",
    "            empty_nuc = list(Gen[f][Gen[f][\"Nuclear_{}\".format(f)]==0].index) # dates of missing nuclear\n",
    "            for k in empty_nuc:\n",
    "                if k not in Empty_Gen[f]: # if not already in missing generation data...\n",
    "                    Empty_Nuc[f].append(k) # ... then append it into list of \"only\" missing nuclear\n",
    "    \n",
    "    return Empty_Gen, Empty_Nuc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort missing generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_missing_generation(Empty_Gen, Empty_Nuc, n_hours=2, add_on=False, is_verbose=False):\n",
    "    \"\"\"\n",
    "    Function that classifies the missing values to prepare for value filling.\n",
    "    Parameter:\n",
    "        Empty_Gen: dict of missing data for all units at one time step.\n",
    "        Empty_Nuc: dict of missing values in nuclear production (for countries with nuclear production)\n",
    "        n_hour: Max number of successive missing hours to be considered as occasional event\n",
    "                (int, default: 2)\n",
    "        add_on: display flourish (bool, default: False)\n",
    "        is_verbose: to display information (bool, default: False)\n",
    "    Return:\n",
    "        2 dict of lists of missing moments and one for missing periods.\n",
    "    \"\"\"\n",
    "    if is_verbose: print(f\"\\t2/{4+int(add_on)} - Sort missing values...\")\n",
    "    ### Distinction between periods of missing data and occasional ones.\n",
    "    Empty_Period = {}\n",
    "    Empty_Unique = {}\n",
    "    \n",
    "    for f in Empty_Gen.keys():\n",
    "        Empty_Period[f] = []\n",
    "        Empty_Unique[f] = []\n",
    "        Empty = sorted(Empty_Gen[f] + Empty_Nuc[f]) # sort all missing datas together (nuclear and not nuclear)\n",
    "        if len(Empty)==0:\n",
    "            pass # if nothing is missing -> nothing is done\n",
    "        elif len(Empty)<=n_hours: # if only few missing dates...\n",
    "            for t in Empty_Gen[f]:\n",
    "                Empty_Unique[f].append(t) # ...everything registered as occasional missing datas.\n",
    "        else:\n",
    "            t,h = 1,0\n",
    "            while t<len(Empty): # look through all datas\n",
    "                if Empty[t]==Empty[h]+pd.Timedelta(\"{} hour\".format(t-h)): # if 2 datas from the same \"period\"\n",
    "                    t+=1 # go for looking if the next date belongs to the period too\n",
    "                else: # if different \"periods\"...\n",
    "                    if t-h>n_hours: # if the period is longer than the limit\n",
    "                        Empty_Period[f].append((Empty[h],t-h)) # Register (start,duration) as a period \n",
    "                    else: # if period is 1 or 2 hours long\n",
    "                        for i in range(t-h):\n",
    "                            Empty_Unique[f].append(Empty[h+i]) # add all investigated datas of this period as occasional missing data\n",
    "                    h += t-h # beginning of another \"period\" investigationo\n",
    "                    t+=1 # hour 0 of the group already seen --> go to possible hour 2.\n",
    "            \n",
    "            if t-h>n_hours: # for the last \"period\", if it is long enough\n",
    "                Empty_Period[f].append((Empty[h],t-h)) # Stock (start,duration) as a period\n",
    "            else: # if period is 1 or 2 hours long\n",
    "                for i in range(t-h):\n",
    "                    Empty_Unique[f].append(Empty[h+i]) # add all investigated datas of this period as occasional missing data\n",
    "    \n",
    "    return Empty_Unique, Empty_Period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill missing generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_generation(table, Empty_Unique, Empty_Period, n_hours=2, days_around=7,add_on=False,\n",
    "                            is_verbose=False):\n",
    "    \"\"\"\n",
    "    Function to fill the missing values in generation data.\n",
    "    Parameter:\n",
    "        table: pandas DataFrame with the generation data.\n",
    "        Empty_Unique: dict of missing values considered as occasional\n",
    "        Empty_Period: dict of missing values considered as a whole period\n",
    "        n_period: Max number of successive missing hours to be considered as occasional event\n",
    "                (int, default: 2)\n",
    "        days_around: number of days after and before a gap to consider to create a 'typical mean day'\n",
    "                (int, default: 7)\n",
    "        add_on: display flourish (bool, default: False)\n",
    "        is_verbose: to display information (bool, default: False)\n",
    "    \"\"\"\n",
    "    if is_verbose: print(f\"\\t3/{4+int(add_on)} - Fill missing values...\")\n",
    "        \n",
    "    ### Filling all missing values\n",
    "    for f in table.keys(): # for all countries\n",
    "        \n",
    "        ### First: fill occasional data\n",
    "        table[f] = fill_occasional(table=table[f], empty=Empty_Unique[f], n_hours=n_hours)\n",
    "        \n",
    "        ### Second: fill periods\n",
    "        table[f] = fill_periods(table=table[f], empty=Empty_Period[f], days_around=days_around)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill occasional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_occasional(table, empty, n_hours=2):\n",
    "    \"\"\"\n",
    "    Function to fill occasional missing data. Only used for missing of 2 missing hours in a row or less.\n",
    "    Parameter:\n",
    "        table: pandas DataFrame with the generation data\n",
    "        empty: the list of empty slots considered as occasional\n",
    "        n_hours: Max number of successive missing hours to be considered as occasional event\n",
    "                    (int, default: 2)\n",
    "    \"\"\"\n",
    "    filled_table = table.copy()\n",
    "    miss = empty.copy() # copy of list of missing dates\n",
    "    \n",
    "    for t in empty: # for all missing dates\n",
    "        col = table.loc[t][table.loc[t]==0].index # list of power station to fill (not always all)\n",
    "        \n",
    "        if t+pd.Timedelta(\"1 hour\") in miss: # If 2 hours in a row\n",
    "            # Replace first missing value with: 1/3 of difference between value(2h after) and value(1h before)\n",
    "            filled_table.loc[t,col] = round((1/3)*filled_table.loc[t+pd.Timedelta(\"2 hour\"),col]\\\n",
    "                    + (2/3)*filled_table.loc[t-pd.Timedelta(\"1 hour\"),col]  , 2) # rounded at 0.01\n",
    "\n",
    "        else: # if single missing hour (or second of a pair) -> linear extrapolation\n",
    "            filled_table.loc[t,col] = (1/2)*(filled_table.loc[t+pd.Timedelta(\"1 hour\"),col] \\\n",
    "                    + filled_table.loc[t-pd.Timedelta(\"1 hour\"),col])\n",
    "\n",
    "        miss.remove(t) # treated hour removed from copied list (to avoid errors)\n",
    "    return filled_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_periods(table, empty, days_around=7):\n",
    "    \"\"\"\n",
    "    Function to fill the missing periods with typical day values\n",
    "    Parameter:\n",
    "        table: pandas DataFrame with the generation data\n",
    "        empty: the list of empty slots considered as periods\n",
    "        days_around: number of days after and before a gap to consider to create a 'typical mean day'\n",
    "                     (int, default: 7)\n",
    "    \"\"\"\n",
    "    filled_table = table.copy()\n",
    "    miss = empty.copy() # copy of the missing periods of the country (not to alter it)\n",
    "    \n",
    "    for t in miss: # for all missing periods\n",
    "        # List of columns to fill (if column is null for the whole duration)\n",
    "        delta = pd.Timedelta(\"{} minutes\".format(15*(t[1]-1))) # entierty of the gap, in minutes\n",
    "        col = filled_table.columns[(filled_table.loc[t[0]:t[0]+delta].sum()==0).values]\n",
    "\n",
    "        # Missing data marked as \"NaN\". Data then not taken into account in the \"typical mean day\"\n",
    "        filled_table.loc[t[0]:t[0]+delta,col]=np.nan\n",
    "\n",
    "        # Creates the \"typical mean day\" (Average of the considered period)\n",
    "        early = t[0]-pd.Timedelta(\"{} days\".format(days_around))\n",
    "        late = t[0]+pd.Timedelta(\"{} days + {} minutes\".format(days_around, 15*(t[1]-1)))\n",
    "        period = filled_table.loc[early:late,col].copy() # surrounding table\n",
    "\n",
    "        typ_day = period.groupby(by=lambda x:(x.hour,x.minute)).mean() # \"typical mean day\"\n",
    "        typ_day.index = pd.date_range(start=\"2018\", periods=typ_day.shape[0], freq=\"15min\")\n",
    "        \n",
    "        for dt in typ_day.index:\n",
    "            loc_large = ((filled_table.index.minute==dt.minute)&(filled_table.index.hour==dt.hour))\n",
    "            localize = filled_table[loc_large].loc[t[0]:t[0]+delta].index\n",
    "            filled_table.loc[localize,col] = typ_day.loc[dt, col]\n",
    "            \n",
    "            \n",
    "    return filled_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_generation(Gen, freq, add_on=False, is_verbose=False):\n",
    "    \"\"\"\n",
    "    Function that resamples the generation data. It can only downsample (lower the resolution) by summing.\n",
    "    \n",
    "    Parameter:\n",
    "        Gen: dict of DataFrames containing the generation data.\n",
    "        freq: the time step (resolution)\n",
    "        add_on: a display flourish (bool, default: False)\n",
    "        is_verbose: to display information (bool, default: False)\n",
    "    \n",
    "    Return:\n",
    "        dict of pandas DataFrame wiht resampled productions\n",
    "    \"\"\"\n",
    "    #######################\n",
    "    ###### Resample Gen.\n",
    "    #######################\n",
    "    if ((check_frequency(freq))&(freq!='15min')):\n",
    "        if is_verbose: print(f\"\\t4/{4+int(add_on)} - Resample Generation data to {freq} steps...\")\n",
    "        for f in Gen.keys(): # For all countries\n",
    "            Gen[f] = Gen[f].resample(freq).sum() # Sum as we talk about energy.\n",
    "            \n",
    "    return Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_exchanges(path_imp, neighbourhood, ctry, start, end, freq='H', is_verbose=False):\n",
    "    \"\"\"\n",
    "    Function to import the cross-border flows.\n",
    "    Finds the useful files to load, load the data, group relevant information and adjust time step.\n",
    "    \n",
    "    Parameter:\n",
    "        path_imp: directory containing the files for cross-border flow data (str)\n",
    "        neighbours: list of useful countries, including neighbour of involved countries (list)\n",
    "        ctry: list of countries to be represented in the simulation\n",
    "        start: starting date (str or datetime)\n",
    "        end: ending date (str or datetime)\n",
    "        freq: time step (str, default: H)\n",
    "        is_verbose: display information (bool, default: False)\n",
    "    \"\"\"\n",
    "    \n",
    "    if is_verbose: print(\"Get and reduce importation data...\")\n",
    "        \n",
    "    ### Files to consider\n",
    "    files = {}\n",
    "    for c in ctry:\n",
    "        try:\n",
    "            files[c] = [f for f in os.listdir(path_imp)\n",
    "                        if ( (f.split(\"_\")[5]==c) & (f.split(\"_\")[0]=='2021'))][0]\n",
    "        except Exception as e:\n",
    "            raise KeyError(f\"No exchange data for {c}: {e}\")\n",
    "\n",
    "\n",
    "    Cross = {} # tables of importation data\n",
    "    for i,c in enumerate(files):# File extraction\n",
    "        if is_verbose: print(\"\\t{}/{} - {}...\".format(i+1,len(files),c))\n",
    "        Cross[c] = pd.read_csv(path_imp+files[c],sep=\";\") # Extraction\n",
    "\n",
    "        # Transform index in time data and convert it from UTC to CET, then keeps only period of interest\n",
    "        Cross[c].index = pd.to_datetime(Cross[c].index,yearfirst=True) # Considered period only\n",
    "        Cross[c] = Cross[c].loc[start:end] # select right period\n",
    "        \n",
    "        ## RESAMPLING hour -> 15min: to be removed\n",
    "        Cross[c] = Cross[c].resample('15min').asfreq(None).interpolate('linear')/4 # basic resample\n",
    "\n",
    "        # Format the import data by selecting and gathering columns\n",
    "        Cross[c] = Cross[c].loc[:,neighbourhood] # Keep only usefull countries\n",
    "        other = [c for c in neighbourhood if c not in ctry] # Label as 'other' all non-main selected countries\n",
    "        Cross[c][\"Other\"] = Cross[c][other].sum(axis=1) # Sum of \"other countries\"\n",
    "        Cross[c].drop(columns=[k for k in neighbourhood if k not in ctry],inplace=True) # Delete details of \"other countries\"\n",
    "        Cross[c].columns = [f\"Mix_{s}_{c}\" for s in Cross[c].columns] # Rename columns\n",
    "    \n",
    "    \n",
    "    ### Resampling the temporal data\n",
    "    if ((is_verbose)&(freq!='15min')): print(f\"Resample Exchanged energy to frequence {freq}...\")\n",
    "    if ((check_frequency(freq))&(freq!='15min')):\n",
    "        for c in Cross.keys(): # For all countries\n",
    "            Cross[c] = Cross[c].resample(freq).sum() # Sum as we talk about energy.\n",
    "            \n",
    "    return Cross"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_exchanges(Cross, sg_data, freq='H'):\n",
    "    \"\"\"\n",
    "    Function to replace the cross-border flow data of ENTSO-E by the cross-border flow data of SwissGrid\n",
    "    \n",
    "    Parameter:\n",
    "        Cross: the Cross-border flow data (dict of pandas DataFrame)\n",
    "        sg_data: information from Swiss Grid (pandas DataFrame)\n",
    "        freq: time step (str, default: H)\n",
    "    \n",
    "    Return:\n",
    "        dict of pandas DataFrame with cross-border flow data for all the countries of the studied area.\n",
    "    \"\"\"    \n",
    "    #### Replace the data in the DataFrames\n",
    "    places = [\"AT\",\"DE\",\"FR\",\"IT\"] # Neighbours of Swizerland (as the function is only for Swizerland)\n",
    "    Exch = {}\n",
    "    for i in Cross.keys():\n",
    "        Exch[i] = Cross[i].copy()\n",
    "    \n",
    "    for c in places:\n",
    "        Exch[\"CH\"][f\"Mix_{c}_CH\"] = sg_data[f\"Mix_{c}_CH\"] # Swiss imprts\n",
    "        Exch[c][f\"Mix_CH_{c}\"] = sg_data[f\"Mix_CH_{c}\"] # Swiss exports\n",
    "    \n",
    "    return Exch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join Generation Exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _join_generation_exchanges(Gen, Cross, is_verbose=False):\n",
    "    \"\"\"Function to join generation and cross-border flow information.\"\"\"\n",
    "    \n",
    "    if is_verbose: print(\"Gather generation and importation...\")\n",
    "    ### Union of all tables of importation and generation together\n",
    "    Union = {}\n",
    "    for f in Gen.keys(): # for all countries\n",
    "        Union[f] = pd.concat([Gen[f],Cross[f]],axis=1) # gathering of the data\n",
    "    \n",
    "    return pd.concat([Union[f] for f in Union.keys()],axis=1) # Join all the tables together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block of function extracts the impact information from the excels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_impacts(ctry, mapping_path, cst_import=False, residual=False, target='CH', is_verbose=False):\n",
    "    \"\"\"\n",
    "    Function to build the impact matrix from mapping stored in files.\n",
    "    Parameter:\n",
    "        ctry: list of countries to load the impacts of (list)\n",
    "        mapping_path: excel file where to find the mapping data (str)\n",
    "        cst_import: whether to consider all impacts of non-traget countres \n",
    "                    as the impact of 'Other' (bool, default: False)\n",
    "        residual: whether to consider production residual for the target country (bool, default: False)\n",
    "        target: the target country (str, default: CH)\n",
    "        is_verbose: to display information (bool, default: False)\n",
    "    \"\"\"\n",
    "    ### Check the country list\n",
    "    if is_verbose: print(\"Extraction of impact vector...\")\n",
    "    # Test the type of country\n",
    "    if type(ctry)==str:\n",
    "        ctry = [ctry]\n",
    "    elif '__iter__' not in dir(ctry):\n",
    "        raise TypeError(\"Parameter ctry should be a list, tuple or str\")\n",
    "    \n",
    "    ### Lists for some formating\n",
    "    old = [\"AT\",\"CH\",\"DE\",\"IT\",\"FR\",\"CZ\"] # countries writen differently in the mapping file\n",
    "    # Wished soure impacts (same order than production data)\n",
    "    expected = pd.Index([\"Other_fossil\",\"Fossil_Gas\",\"Fossil_Peat\",\"Biomass\",\n",
    "                            \"Hydro_Run-of-river_and_poundage\",\"Solar\",\"Waste\",\"Wind_Onshore\",\n",
    "                            \"Other_renewable\",\"Fossil_Oil_shale\",\"Hydro_Water_Reservoir\",\n",
    "                            \"Fossil_Brown_coal/Lignite\",\"Nuclear\",\"Fossil_Oil\",\"Hydro_Pumped_Storage\",\n",
    "                            \"Wind_Offshore\",\"Fossil_Hard_coal\",\"Geothermal\",\n",
    "                            \"Fossil_Coal-derived_gas\",\"Marine\"])\n",
    "    \n",
    "    ### Extract the impact information\n",
    "    impacts = {}\n",
    "    \n",
    "    if is_verbose: print(\"\\t. Mix_Other \",end=\"\") # Mix from other countries\n",
    "    impacts['Other'] = other_from_excel(mapping=mapping_path)\n",
    "    \n",
    "    for c in ctry:\n",
    "        if is_verbose: print(f\"/ {c} \",end=\"\")\n",
    "        imp = country_from_excel(mapping=mapping_path, place=c, is_old=(c in old))\n",
    "        if imp is not None:\n",
    "            impacts[c] = shape_country(imp, place=c, is_old=(c in old), imp_other=impacts['Other'],\n",
    "                                       cst_import=((cst_import)&(c!=target)), expected=expected)\n",
    "            \n",
    "    ### Add impact of residual\n",
    "    if residual: # Mix from the residual part -> direct after \"Mix_Other\" (residual only in CH)\n",
    "        if is_verbose: print(\"+ Residual \",end=\"\")\n",
    "        impacts['CH'] = residual_from_excel(impact_ch=impacts['CH'],mapping=mapping_path)\n",
    "        \n",
    "    ### Gather impacts in one table\n",
    "    if is_verbose: print(\".\")\n",
    "    impact_matrix = pd.concat([impacts[c] for c in impacts.keys()])\n",
    "    \n",
    "    \n",
    "    return impact_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other from excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_from_excel(mapping):\n",
    "    \"\"\"Load the mapping for 'Other' from an excel file (mapping).\"\"\"\n",
    "    ### Impact for production mix of 'other countries'\n",
    "    indicators = [\"GWP\",\"CED_renewable\",\"CED_non-renewable\",\"ES2013\"]\n",
    "    d = pd.read_excel(mapping,sheet_name=\"ENTSOE_avg\",\n",
    "                         header=1, usecols=np.arange(2,7), index_col=[0]) # extract\n",
    "    return pd.DataFrame(d.loc['ENTSOE',:].values,index=indicators, columns=[\"Mix_Other\"]).T # format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Country form excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_from_excel(mapping, place, is_old=True):\n",
    "    \"\"\"Load the mapping of a given country (place) from an excel file (mapping).\"\"\"\n",
    "    try: # test if the country is available in the mapping file\n",
    "        if is_old: \n",
    "            d = pd.read_excel(mapping,sheet_name=place, index_col=[0,1,2]) # import\n",
    "        else:\n",
    "            d = pd.read_excel(mapping,sheet_name=place) # import\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Mapping for {place} not available: {e} \")\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_country(d, place, expected, is_old=True, imp_other=None, cst_import=False):\n",
    "    \"\"\"\n",
    "    Bring some changes in the index and column namings and order for the impact matrix.\n",
    "    Parameter:\n",
    "        d: the impact matrix for one given country (pandas DataFrame)\n",
    "        place: the country (str)\n",
    "        expected: list of expected labels for the production means\n",
    "        is_old: if the country belongs to the ones with an old formating (bool, default True)\n",
    "        imp_other: the impact matrix for 'Other' countries (pandas DataFrame, default None)\n",
    "        cst_import: whether to consider all impacts as the impact of 'Other' (bool, default: False)\n",
    "    \"\"\"\n",
    "    col_id = len(d.loc[:,:\"Environmental impacts of ENTSO-E sources\"].columns)-1 # columns to consider\n",
    "\n",
    "    # Select corresponding data & rename columns\n",
    "    imp_ctry = pd.DataFrame(d.iloc[:,col_id:col_id+4].dropna().values)\n",
    "    imp_ctry.drop(index=[0,1,2],inplace=True)\n",
    "    imp_ctry.columns = [\"GWP\",\"CED_renewable\",\"CED_non-renewable\",\"ES2013\"]\n",
    "\n",
    "    # Prepare index labels\n",
    "    if is_old: # find right index labels\n",
    "        ind = list(d.index.get_level_values(0)[1:].dropna().drop_duplicates() + \" \" + place)\n",
    "    else:\n",
    "        ind = list(d.iloc[:,[0,col_id]].dropna().iloc[:,0] + \" \" + place)\n",
    "\n",
    "    for i in range(len(ind)): # Precise the country and change writing details\n",
    "        ind[i] = ind[i].replace(\")\",\"\").replace(\"(\",\"\").split(\" \")\n",
    "        if \"\" in ind[i]:\n",
    "            ind[i].remove(\"\")\n",
    "        ind[i] = \"_\".join(ind[i]).replace(\"_Fossil\",\"_fossil\")\n",
    "\n",
    "    if is_old: # remove some lines if needed\n",
    "        ind = ind[:ind.index(\"Solar_{}\".format(place))+1]\n",
    "        imp_ctry.index = ind[1:] # write the indexes in the table\n",
    "\n",
    "    else:\n",
    "        imp_ctry.index = ind # write the indexes in the table\n",
    "    imp_ctry.replace(\"-\",0,inplace=True) # replace missing datas for computer understanding\n",
    "\n",
    "    if cst_import: # put every value to constant like \"Other\"\n",
    "        for k in imp_ctry.columns:\n",
    "            imp_ctry.loc[:,k] = imp_other[k].values[0]\n",
    "\n",
    "    return imp_ctry.reindex(expected+\"_\"+place,fill_value=0) # Set data in the right order + fill missing lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual from excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_from_excel(impact_ch, mapping):\n",
    "    \"\"\"\n",
    "    Load impact data of the production residual and add it to the impact matrix.\n",
    "    Parameter:\n",
    "        impact_ch: impact matrix of production means of Swizerland (pandas DataFrame)\n",
    "        mapping: path to file with the mapping (str)\n",
    "    Return:\n",
    "        pandas DataFrame with the impact_ch, where the impact of residual production is added.\n",
    "    \"\"\"\n",
    "    ### Addition of the residual data\n",
    "    imp = impact_ch.copy()\n",
    "    try: # test if the \"country\" is available in the mapping file\n",
    "        d = pd.read_excel(mapping,sheet_name=\"Residue\",index_col=0) # import\n",
    "        # select\n",
    "        d = pd.DataFrame(d.loc[[\"Residue_Hydro\",\"Residue_Other\"],\n",
    "                               \"Environmental impacts of ENTSO-E sources\":].values,\n",
    "                         columns=impact_ch.columns,\n",
    "                         index=[\"Residual_Hydro_CH\",\"Residual_Other_CH\"])\n",
    "        \n",
    "        imp = pd.concat([d, imp],axis=0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        raise ValueError(f\" Residual not available: {e}\")\n",
    "    \n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import annex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SwissGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_swissGrid(path_sg, start, end, freq='H'):\n",
    "    \"\"\"\n",
    "    Function to load production and cross-border flows information from Swiss Grid. Data used many times\n",
    "    along the algorithm.\n",
    "    Parameter:\n",
    "        path_sg: path to the file with Swiss Grid information (str)\n",
    "        start: starting date (datetime or str)\n",
    "        end: ending date (datetime or str)\n",
    "        freq: time step (str, default H)\n",
    "    Return:\n",
    "        pandas DataFrame with SwissGrid information in MWh and at the good time step.\n",
    "    \"\"\"\n",
    "    ### Default path\n",
    "    if path_sg is None:\n",
    "        path_sg = get_default_file(name='SwissGrid_total.csv')\n",
    "    \n",
    "    ### Import SwissGrid data\n",
    "    parser = lambda x: (pd.to_datetime(x, format='%d.%m.%Y %H:%M')\n",
    "                        - pd.Timedelta(\"15min\")) # starts at 00:00 (not 00:15)\n",
    "    sg = pd.read_csv(path_sg, sep=\";\",index_col=0, parse_dates=[0], date_parser=parser)\n",
    "    sg = sg.drop(columns=[\"Consommation_CH\",\"Consommation_Brut_CH\"]) # Remove unused columns\n",
    "\n",
    "    ### Check info availability (/!\\ if sg smaller, big problem not filled yet !!!)\n",
    "    if 'Production_CH' not in sg.columns:\n",
    "        raise KeyError(\"Missing information 'Production_CH' in SwissGrid Data.\")\n",
    "    if ((start<sg.index[0])|(end>sg.index[-1])): # print information only\n",
    "        warning = \"Resudual computed only during {} - {}. SwissGrid Data not available on the rest of the period.\"\n",
    "        print(warning.format(sg.loc[start:end].index[0],sg.loc[start:end].index[-1]))\n",
    "        \n",
    "    ### Rename the columns\n",
    "    sg.columns = [\"Production_CH\",\"Mix_CH_AT\",\"Mix_AT_CH\",\"Mix_CH_DE\",\"Mix_DE_CH\",\n",
    "                  \"Mix_CH_FR\",\"Mix_FR_CH\",\"Mix_CH_IT\",\"Mix_IT_CH\"]\n",
    "\n",
    "    ### Select the interesting data, resample to right frequency and convert kWh -> MWh\n",
    "    return sg.loc[start:end,:].resample(freq).sum() / 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load useful countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_useful_countries(path_neighbour, ctry):\n",
    "    \"\"\"\n",
    "    Function to load a list of countries directly or indirectly involved in the computation.\n",
    "    Countries directly involved are passed as arguments. Countries indirectly involved are their\n",
    "    neighbours. These indirectly involved countries help building the import from 'other' countries.\n",
    "    \"\"\"\n",
    "    ### Default path\n",
    "    if path_neighbour is None:\n",
    "        path_neighbour = get_default_file(name='Neighbourhood_EU.csv')\n",
    "    \n",
    "    ### For importing only the usefull data\n",
    "    neighbouring = pd.read_csv(path_neighbour,sep=\";\",index_col=0)\n",
    "    useful = list(ctry) # List of countries considered + neighbours\n",
    "    for c in ctry:\n",
    "        useful += list(neighbouring.loc[c].dropna().values)\n",
    "    useful = list(np.unique(useful)) # List of the useful countries, one time each.\n",
    "    return useful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GridLosses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grid_losses(network_loss_path, start, end):\n",
    "    \"\"\"\n",
    "    Function that loads network grid losses and returns a pandas DataFrame with the fraction of\n",
    "    network loss in the transmitted electricity for each month.\n",
    "    \"\"\"\n",
    "    ### Default path\n",
    "    if network_loss_path is None:\n",
    "        network_loss_path = get_default_file(name='Pertes_OFEN.csv')\n",
    "    \n",
    "    # Get and calculate new power demand for the FU vector\n",
    "    losses = pd.read_csv(network_loss_path, sep=\";\")\n",
    "    losses['Rate'] = 1 + (losses.loc[:,'Pertes']/losses.loc[:,'Conso_CH'])\n",
    "\n",
    "    localize = ((losses.annee>=start.year) & (losses.annee<=end.year))\n",
    "    output = losses.loc[localize, ['annee','mois','Rate']].rename(columns={'annee':'year', 'mois':'month'})\n",
    "    return output.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load gap content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gap_content(path_gap, start, end, freq='H', header=59):\n",
    "    \"\"\"\n",
    "    Function that defines the relative composition of the swiss residual production. The function is very\n",
    "    file format specific.\n",
    "    Parameter:\n",
    "        path_gap: path to the file containing residual content information (str)\n",
    "        start: starting date (datetime or str)\n",
    "        end: ending date (datetime or str)\n",
    "        freq: time step (str, default H)\n",
    "        header: row in the file to use as header (int, default 59)\n",
    "    Return:\n",
    "        pandas DataFrame with relative residual production composition for each time step.\n",
    "    \"\"\"\n",
    "    ### Default path\n",
    "    if path_gap is None:\n",
    "        path_gap = get_default_file(name='Repartition_Residus.xlsx')\n",
    "    \n",
    "    interest = ['Centrales au fil de l’eau','Centrales therm. classiques et renouvelables']\n",
    "    df = pd.read_excel(path_gap, header=header, index_col=0).loc[interest].T\n",
    "    df[\"Hydro_Res\"] = df['Centrales au fil de l’eau']/df.sum(axis=1) # calculate the % part of each potential source\n",
    "    df[\"Other_Res\"] = 1-df.loc[:,'Hydro_Res']\n",
    "    \n",
    "    df.index = pd.to_datetime(df.index,yearfirst=True) # time data\n",
    "    \n",
    "    ###########################\n",
    "    ##### Adapt the time resolution of raw data\n",
    "    #####\n",
    "    # If year or month -> resample at start ('S') of month/year with average of info\n",
    "    localFreq = freq # copy frequency\n",
    "    if freq[0] in [\"M\",\"Y\"]:\n",
    "        localFreq = freq[0]+\"S\" # specify at 'start'\n",
    "        df = df.resample(localFreq).mean()\n",
    "    # If in week -> resample with average\n",
    "    elif freq in ['W','w']:\n",
    "        localFreq = 'd' # set local freq to day (to later sum in weeks)\n",
    "\n",
    "    ###############################\n",
    "    ##### Select information\n",
    "    #####\n",
    "    res_start = start + pd.offsets.MonthBegin(-1) # Round at 1 month before start\n",
    "    res_end = end + pd.offsets.MonthEnd(0) # Round at the end of the last month\n",
    "    \n",
    "    df = df.loc[res_start:res_end, ['Hydro_Res','Other_Res']] # Select information only for good duration\n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    ##### Build the adapted time series with right time step\n",
    "    #####\n",
    "    gap = pd.DataFrame(None, columns=df.columns,\n",
    "                       index = pd.date_range(start=res_start,\n",
    "                                             end=max(res_end, df.index[-1]), freq=localFreq))\n",
    "\n",
    "    if localFreq[0]=='Y':\n",
    "        for dt in df.index:\n",
    "            localize = (gap.index.year==dt.year)\n",
    "            gap.loc[localize,:] = df.loc[dt,:].values\n",
    "\n",
    "    elif localFreq[0]==\"M\":\n",
    "        for dt in df.index:\n",
    "            localize = ((gap.index.year==dt.year)&(gap.index.month==dt.month))\n",
    "            gap.loc[localize,:] = df.loc[dt,:].values\n",
    "\n",
    "    else:\n",
    "        for dt in df.index: # everything from (week, ) day to 15 minutes\n",
    "            if dt.dayofweek<=4: # week day\n",
    "                localize = ((gap.index.year==dt.year)&(gap.index.month==dt.month)\n",
    "                            &(gap.index.dayofweek<=4))\n",
    "            else:\n",
    "                localize = ((gap.index.year==dt.year)&(gap.index.month==dt.month)\n",
    "                            &(gap.index.dayofweek==dt.dayofweek))\n",
    "            gap.loc[localize,:] = df.loc[dt,:].values\n",
    "        gap = gap.dropna(axis=0)\n",
    "        \n",
    "        if freq in [\"W\",\"w\"]: # Aggregate into weeks\n",
    "            gap = gap.fillna(method='ffill').resample(freq).mean()\n",
    "\n",
    "    return gap.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw Entso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rawEntso(mix_data, freq='H'):\n",
    "    \"\"\"\n",
    "    Function that can load an existing production and exchange matrix in a CSV file\n",
    "    \"\"\"\n",
    "    ################################################\n",
    "    # Labeling of data matrix and import of data\n",
    "    ################################################\n",
    "    if type(mix_data)==str: # Import from file\n",
    "        check_frequency(freq) # Check the frequency\n",
    "        tPass = {'15min':'15min','30min':'30min',\"H\":\"hour\",\"D\":\"day\",'d':'day','W':\"week\",\n",
    "                 \"w\":\"week\",\"MS\":\"month\",\"M\":\"month\",\"YS\":\"year\",\"Y\":\"year\"}\n",
    "        \n",
    "        data = pd.read_csv(mix_data+f\"ProdExchange_{tPass[freq]}.csv\",sep=\";\",\n",
    "                               index_col=0, parse_dates=[0])\n",
    "            \n",
    "    elif type(mix_data)==pd.core.frame.DataFrame: # import from the DataFrame passed as argument\n",
    "        data = mix_data\n",
    "        \n",
    "    else: raise KeyError(f\"Data type {type(mix_data)} for raw_prodExch is not supported.\")\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get default file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_file(name,level=2):\n",
    "    \"\"\"Function to load a default file form directory 'support_file'\"\"\"\n",
    "    ### Default RELATIVE path (indepenently of the file structure)\n",
    "    path = os.path.abspath(r\"{}\".format(__file__)).split(\"/\")[:-level] # List to main directory of EcoDyn\n",
    "    path = path + ['support_files',name] # add the default SwissGrid file\n",
    "    if os.path.isfile(r\"{}\".format(\"/\".join(path))):\n",
    "        return r\"{}\".format(\"/\".join(path)) # Recreate the file address\n",
    "    elif level==2:\n",
    "        return get_default_file(name,level=1)\n",
    "    else:\n",
    "        raise KeyError(f\"Default support file {name} not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mix computation Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Track mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_mix(raw_data=None, freq='H', network_losses=None, target=None, residual_global=False,\n",
    "              net_exchange=False, is_verbose=False):\n",
    "    \"\"\"Master function for the electricity mix computation.\n",
    "    Parameter:\n",
    "        data_path: path to entso-e data (str), or data Frame. If None, load default data. Default: None\n",
    "        freq: time step (str). Default: 'H'\n",
    "        network_loss_path: path to data giving network losses (str) If None, load default data. Default: None\n",
    "        target: the studied country (str). Default: 'CH'\n",
    "        residual_global: if swiss residual data was included at transmission level\n",
    "        net_exchange: to consider net cross-border flows (bool). Default: False (total bi-directional flows)\n",
    "        path: the path where to save (str). Default: None\n",
    "        is_verbose: show text during computation.\n",
    "    Return\n",
    "        pandas DataFrame containing the electricity mix of the studied country.\"\"\"\n",
    "    \n",
    "    t0 = time() # time measurment\n",
    "    \n",
    "    if is_verbose: print(\"Importing information...\")\n",
    "    df = load_rawEntso(mix_data=raw_data, freq=freq)\n",
    "    ctry, ctry_mix, prod_means, all_sources = reorder_info(data=df)\n",
    "    \n",
    "    \n",
    "    if net_exchange:\n",
    "        df = create_net_exchange(df, ctry=ctry)\n",
    "        \n",
    "    if network_losses is not None:\n",
    "        uP = get_grid_losses(df, losses=network_losses)\n",
    "    else:\n",
    "        uP = pd.Series(data=1,index=df.index) # Grid losses not considered -> 1\n",
    "    \n",
    "    u = set_FU_vector(all_sources=all_sources, target=target)\n",
    "        \n",
    "    \n",
    "    \n",
    "    if is_verbose: print(\"Tracking origin of electricity...\")\n",
    "    mixE = compute_tracking(df, all_sources=all_sources, u=u, uP=uP, ctry=ctry, ctry_mix=ctry_mix,\n",
    "                            prod_means=prod_means, residual=residual_global,freq=freq,\n",
    "                            is_verbose=is_verbose)\n",
    "    \n",
    "\n",
    "    if is_verbose: print(\"\\n\\tElectricity tracking: {:.1f} sec.\\n\".format(time()-t0))\n",
    "    return mixE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reorder info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_info(data):\n",
    "    \"\"\"\n",
    "    Function to rename and reorder the columns in the production and exchanges table. It returns 4 useful\n",
    "    lists for the electricity tracking.\n",
    "    Parameter:\n",
    "        data: the production and exchange table (pandas DataFrame)\n",
    "    Return:\n",
    "        ctry: sorted list of involved countries\n",
    "        ctry_mix: list of countries where eletricity can come from, including 'Other' (list)\n",
    "        prod_means: list of production means, without mixes (list)\n",
    "        all_sources: list of production means and mixes, with precision of the country of origin (list)\n",
    "    \"\"\"\n",
    "    # Reorganize columns in the dataset\n",
    "    ctry = sorted(list(np.unique([k.split(\"_\")[-1] for k in data.columns])))# List of considered countries\n",
    "    ctry_mix = list(np.unique([k.split(\"_\")[1] for k in data.columns if k[:3]==\"Mix\"])) # List of importing countries (right order)\n",
    "    ctry_mix = ctry +[k for k in ctry_mix if k not in ctry] # add \"Others\" in the end of pays_mixe\n",
    "\n",
    "    # Definition of the means of production and column names for the calculation matrix\n",
    "    prod_means = []\n",
    "    all_sources = []\n",
    "    for k in data.columns:\n",
    "        if k.split(\"_\")[-1]==ctry[0]: # Gather all energy source names (only for one country)\n",
    "            if k[:3]!=\"Mix\":\n",
    "                prod_means.append(k.split(\"_{}\".format(ctry[0]))[0])\n",
    "            elif k[:3]==\"Mix\":\n",
    "                prod_means.append(\"_\".join(k.split(\"_\")[:-1])) # Energy exchanges\n",
    "                all_sources.append(\"_\".join(k.split(\"_\")[:-1]))\n",
    "\n",
    "    all_sources += [k for k in data.columns if k[:3]!=\"Mix\"] # Add AFTER the names of means of production\n",
    "    \n",
    "    return ctry, ctry_mix, prod_means, all_sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create net exchanges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_net_exchange(data, ctry):\n",
    "    \"\"\"\n",
    "    Adapt the cross-border flow to consider exchanges at each border and time step as net.\n",
    "    Net exchange means that electricity can only go from A to B or from B to A, but not in \n",
    "    both directions at the same time.\n",
    "    \"\"\"\n",
    "    d = data.copy()\n",
    "    # Correction of the cross-border (turn into net exchanges) over each time step\n",
    "    for i in range(len(ctry)):\n",
    "        for j in range(len(ctry)-1,i,-1):\n",
    "            \n",
    "            decide = (d[f\"Mix_{ctry[i]}_{ctry[j]}\"] >= d[f\"Mix_{ctry[j]}_{ctry[i]}\"]) # direction\n",
    "            diff = d[f\"Mix_{ctry[i]}_{ctry[j]}\"] - d[f\"Mix_{ctry[j]}_{ctry[i]}\"] # exchange difference\n",
    "            d.loc[:,f\"Mix_{ctry[i]}_{ctry[j]}\"] = decide*diff # if flow from i to j --> + value\n",
    "            d.loc[:,f\"Mix_{ctry[j]}_{ctry[i]}\"] = (decide-1)*diff # if from j to i <-- -value\n",
    "\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get grid losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grid_losses(data, losses=None):\n",
    "    \"\"\"Gives for each time step the amount of electricity to produce in order to consume 1 kWh.\"\"\"\n",
    "    # Add new demand in the FU vector for each step of time\n",
    "    uP = pd.Series(data=None,index=data.index, dtype='float32') # vector for values of FU vector at each time step\n",
    "    for k in losses.index: # grid losses ratio for each step of time\n",
    "        localize = ((uP.index.year==losses.loc[k,\"year\"])&(uP.index.month==losses.loc[k,\"month\"]))\n",
    "        uP.iloc[localize] = losses.loc[k,\"Rate\"]\n",
    "        \n",
    "    return uP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set FU vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_FU_vector(all_sources, target='CH'):\n",
    "    \"\"\"Defines the Functional Unit vector: full of zeros, except at the indexes corresponding to the target\n",
    "    country, where a 1 is written.\"\"\"\n",
    "    # Defines the FU vector\n",
    "    u = np.zeros( len(all_sources) ) # basic Fonctional Unit Vector (FU vector) --> do never change. Is multiplied by uP (for losses) during process\n",
    "    u[all_sources.index(f\"Mix_{target}\")] = 1 # Location of target country in the FU vector\n",
    "    return u"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def compute_tracking(data, all_sources, u, uP, ctry, ctry_mix, prod_means,\n",
    "                     residual=False, freq='H', is_verbose=False):\n",
    "    \"\"\"Function leading the electricity tracking: by building the technology matrix, computing the\n",
    "    inversion and selecting of the information for the target country.\n",
    "    \n",
    "    Parameter:\n",
    "        data: Table with the production and exchange data of all involved countries (pandas DataFrame)\n",
    "        all_sources: an ordered list with the mix names and production mean names, without origin (list)\n",
    "        u: functional unit vector, full of zeros and with ones for the targeted countries (list)\n",
    "        uP: vector that indicates the amount of energy before losses to obtain 1kWh of consumable elec (list)\n",
    "        ctry: sorted list of involved countries (list)\n",
    "        ctry_mix: list of countries where eletricity can come from, including 'Other' (list)\n",
    "        prod_means: list of production means, without mixes (list)\n",
    "        residual: if residual are considered (bool, default: False)\n",
    "        freq: the time step (str, default: H)\n",
    "        is_verbose: to display information (bool, default: False)\n",
    "    \n",
    "    Return:\n",
    "        pandas DataFrame with the electricity mix in the target country.\n",
    "    \"\"\"\n",
    "    mixE = pd.DataFrame(data=None,index=data.index,columns=all_sources) # Output DataFrame\n",
    "    \n",
    "    if is_verbose:\n",
    "        check_frequency(freq)\n",
    "        step = {'15min':96, '30min':48, 'H':24, 'd':7, 'D':7,\n",
    "                'W':1,'w':1,'M':1, 'MS':1, 'Y':1, 'YS':1}[freq]\n",
    "        step_name = {'15min':\"day\", '30min':\"day\", 'H':\"day\", 'd':\"week\", 'D':\"week\",\n",
    "                     'W':'week', 'w':'week','M':\"month\", 'MS':\"month\", 'Y':\"year\", 'YS':\"year\"}[freq]\n",
    "        total = np.ceil(mixE.shape[0]/step).astype('int32') # total nb of steps to display\n",
    "    else:\n",
    "        step = mixE.shape[0]\n",
    "    \n",
    "        \n",
    "    # For each considered step of time\n",
    "    for t in range(mixE.shape[0]):\n",
    "        \n",
    "        if ((is_verbose)&(t%step==0)):\n",
    "            print(f\"\\tcompute for {step_name} {(t//step)+1}/{total}   \", end=\"\\r\")\n",
    "        \n",
    "        ##############################################\n",
    "        # Build the technology matrix A\n",
    "        ##############################################\n",
    "        A = build_technology_matrix(data.iloc[t], ctry, ctry_mix, prod_means)\n",
    "        L = A.shape[0]\n",
    "        \n",
    "        #######################################################\n",
    "        # Drop the empty columns and lines for easier inversion\n",
    "        #######################################################\n",
    "        A, presence = clean_technology_matrix(A)\n",
    "        \n",
    "        #########################################################\n",
    "        # Inversion & reintegrtion of the empty lines and columns\n",
    "        #########################################################\n",
    "        Ainv = invert_technology_matrix(A, presence, L=L)\n",
    "        \n",
    "        #################################\n",
    "        # Select only target country\n",
    "        #################################\n",
    "        # Extraction of the target country (multiply Ainv, basic FU vector and losses rate for the step of time)\n",
    "        mixE.iloc[t,:] = np.dot(Ainv, u*uP.iloc[t]) # Extract for target country and store it in the output table\n",
    "    \n",
    "    #######################################################################\n",
    "    # Clear columns related to residual in other countries than CH\n",
    "    #######################################################################\n",
    "\n",
    "    # Possibly non-used residue columns are deleted (Only residual for CH can be considered)\n",
    "    if residual:\n",
    "        mixE = mixE.drop(columns=[k for k in mixE.columns\n",
    "                                  if ((k.split(\"_\")[0]==\"Residual\")&(k[-3:]!=\"_CH\"))])\n",
    "\n",
    "    return mixE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build technology matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_technology_matrix(data, ctry, ctry_mix, prod_means):\n",
    "    \"\"\"Function building the technology matrix based on the production and exchange data.\n",
    "    Parameter:\n",
    "        data: the production and exchange data (pandas DataFrame)\n",
    "        ctry: sorted list of involved countries (list)\n",
    "        ctry_mix: list of countries where eletricity can come from, including 'Other' (list)\n",
    "        prod_means: list of production means, without mixes (list)\n",
    "    Return:\n",
    "        numpy array representing the technology matrix A\n",
    "    \"\"\"\n",
    "    # Gathering production by country in a matrix\n",
    "    energy = pd.DataFrame(data=data.values.reshape(( len(ctry) , len(prod_means) )),\n",
    "                           columns=prod_means, index=ctry, dtype='float32')\n",
    "\n",
    "    # Compute the contribution rate of each production unit in the production mix of each country\n",
    "    weight = energy.div(energy.sum(axis=1),axis=0)\n",
    "    \n",
    "    # Shape parameters\n",
    "    cm = 0 # anchor column number for the blocks containing data\n",
    "    cM = len(ctry_mix) # width of the block containing data\n",
    "    height =  len(prod_means) - len(ctry_mix) # height data block with generation without exchange\n",
    "    L = len(ctry_mix) + height*len(ctry) # Shape of technology matrix\n",
    "    \n",
    "    # Building and calculation of the technology matrix A for this specific step of time\n",
    "    # shapes of the A matrix\n",
    "    A = np.zeros((L,L))\n",
    "\n",
    "    # set production data one country after another\n",
    "    for i in range( len(ctry) ):\n",
    "        # Calculate appropriate position in A\n",
    "        i_mix=ctry_mix.index(ctry[i]) # indices of the location's order of countries\n",
    "        lm = cM + i*( height ) # upper limit of the cosidered data block\n",
    "        lM = lm+( height ) # lower limit of the cosidered data block\n",
    "        # Replacement\n",
    "        A[lm:lM,cm+i_mix] = weight.loc[ctry[i]].iloc[:height].values # Column by column\n",
    "\n",
    "    # set link between mixes (contribution of a mix to another --> cross-border flows contribution)\n",
    "    A[cm:cM,cm:cM-1] = weight.loc[ctry,[f\"Mix_{k}\" for k in ctry_mix]].T.values\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean technology matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_technology_matrix(A):\n",
    "    \"\"\"Reduce the size of the technology matrix. As the matrix A is a square matrix, for all indexes i\n",
    "    where the i-th row AND the i-th column are both full of zeros, both row and column i are dropped.\n",
    "    All other indexes j are written in the list 'presence', and the row and column j is kept in A.\"\"\"\n",
    "    ###############################################\n",
    "    # drop the empty columns and line for inversion\n",
    "    ###############################################\n",
    "    presence_line = pd.Series(A.sum(axis=1)!=0) # The lines not full of zeros (true or false)\n",
    "    presence_cols = pd.Series(A.sum(axis=0)!=0) # The columns not full of zeros (true or false)\n",
    "    \n",
    "    presence = np.logical_or(presence_line, presence_cols) # keep if value on a line or column\n",
    "    presence = presence[presence.values==True].index # lines and columns to keep (indexes)\n",
    "    A = A[presence,:][:,presence] # select only the non-empty lines and columns\n",
    "    \n",
    "    return A, presence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invert technology matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_technology_matrix(A, presence, L):\n",
    "    \"\"\"Track the electric mix: it consists in computing (Id - A)⁻¹\n",
    "    Parameter:\n",
    "        A: technology matrix (numpy array)\n",
    "        presence: list of indexes to replace the computation results in their context\n",
    "        L: the size of the results (and original A, before it was cleaned)\n",
    "    Return:\n",
    "        numpy array of shape (L, L)\n",
    "    \"\"\"\n",
    "    ##########################################################\n",
    "    # Inversion & reintegrtion of the empty lines and columns\n",
    "    #########################################################\n",
    "    Ainv = np.zeros((L,L)) # storage matrix\n",
    "    m = np.linalg.inv(np.eye(len(presence)) - A) # inversion\n",
    "    k_m = 0\n",
    "    for i in presence:\n",
    "        Ainv[i,presence] = m[k_m] # set for each concerned line the columns to fill\n",
    "        k_m+=1\n",
    "    \n",
    "    return Ainv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact computation Group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_impacts(mix_data, impact_data, freq='H', is_verbose=False):\n",
    "    \"\"\"Computes the impacts based on electric mix and production means impacts.\n",
    "    Parameter:\n",
    "        mix_data: information about the electric mix in the target country (pandas DataFrame)\n",
    "        impact_data: impact matrix for all production units (pandas DataFrame)\n",
    "        freq: time step of the study (str, default: H)\n",
    "        is_verbose: to display information (bool, default: False)\n",
    "    Return:\n",
    "        dict of pandas DataFrame containing the impacts.\"\"\"\n",
    "    check_frequency(freq)\n",
    "    \n",
    "    t3 = time()\n",
    "    \n",
    "    impacts_matrix = adapt_impacts(impact_data, production_units=mix_data.columns)\n",
    "    \n",
    "    if is_verbose: print(\"Compute the electricity impacts...\\n\\tGlobal...\")\n",
    "    collect_impacts = {}\n",
    "    collect_impacts['Global'] = compute_global_impacts(mix_data=mix_data, impact_data=impacts_matrix,\n",
    "                                                       freq=freq)\n",
    "    \n",
    "    \n",
    "    for i in [\"GWP\",\"CED_renewable\",\"CED_non-renewable\",\"ES2013\"]:\n",
    "        if is_verbose: print(\"\\t{}...\".format(i))\n",
    "        collect_impacts[i] = compute_detailed_impacts(mix_data=mix_data, impact_data=impacts_matrix.loc[:,i],\n",
    "                                                      indicator=i, freq=freq)\n",
    "    \n",
    "    if is_verbose: print(\"Impact computation: {} sec.\".format(round(time()-t3,1))) # time report\n",
    "\n",
    "    return collect_impacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adapt_impacts(impact_data=None, production_units=None):\n",
    "    \"\"\"Adapt the mix data if there is a residual to consider.\"\"\"\n",
    "    impact = impact_data.copy()\n",
    "\n",
    "    # adapt the impact data to the production unit for Residual\n",
    "    if \"Residual_Other_CH\" not in production_units:\n",
    "        if \"Residual_Other_CH\" in impact.index:\n",
    "            impact = impact.drop(index=\"Residual_Other_CH\") # remove from the impacts if not existing in mix\n",
    "    \n",
    "    return impact"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute global impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_global_impacts(mix_data, impact_data=None, freq='H'):\n",
    "    \"\"\"Computes the overall impacts of electricity for each indicator\"\"\"\n",
    "    ###############################################\n",
    "    # Computation of global impact\n",
    "    ###############################################\n",
    "\n",
    "    # All production units and the \"other countries\" are considered\n",
    "    mix = mix_data.drop(columns=[k for k in mix_data.columns\n",
    "                                 if ((k.split(\"_\")[0]==\"Mix\")&(k.find(\"Other\")==-1))]) # delete \"Mix\"\n",
    "\n",
    "    \n",
    "    # Compute the impacts\n",
    "    pollution = pd.DataFrame(np.dot(mix.values,impact_data.values),\n",
    "                             index=mix.index,columns=impact_data.columns)\n",
    "\n",
    "    return pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute detailed impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_detailed_impacts(mix_data, impact_data, indicator, freq='H'):\n",
    "    \"\"\"Computes the impacts of electricity per production unit for a given indicator\"\"\"\n",
    "    #####################################################\n",
    "    # Computation of detailed impacts per production unit\n",
    "    #####################################################\n",
    "\n",
    "    # All production units and the \"other countries\" are considered\n",
    "    mix = mix_data.drop(columns=[k for k in mix_data.columns\n",
    "                                 if ((k.split(\"_\")[0]==\"Mix\")&(k.find(\"Other\")==-1))]) # delete \"Mix\"\n",
    "    \n",
    "    # Impact data already charged & grid data already without useless \"Mix\" columns\n",
    "    pollution = pd.DataFrame(np.dot(mix,np.diag(impact_data)),\n",
    "                             columns=impact_data.index, index=mix.index) # Calculation & storage\n",
    "    pollution.rename_axis(\"{}_source\".format(indicator),\n",
    "                          axis=\"columns\",inplace=True) # Rename the main axis of the table\n",
    "    \n",
    "    return pollution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handle Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_residual(prod, sg_data, gap=None):\n",
    "    \"\"\"\n",
    "    Function to insert the residue as a swiss production high voltage. Two residues are considered: hydro run off and the rest.\n",
    "    \n",
    "    Parameter:\n",
    "        - prod: the production mix of Swizerland [pandas DataFrame] (Date should be as index)\n",
    "        - sg_data: information from SwissGrid (pandas DataFrame)\n",
    "        - gap: information about the nature of the residual (pandas DataFrame)\n",
    "    \n",
    "    Return:\n",
    "        production mix with the Residue [pandas DataFrame]\n",
    "    \"\"\"\n",
    "    ### Calculation of global resudual\n",
    "    all_prod = prod.copy()\n",
    "    init_cols = list(prod.columns)\n",
    "    \n",
    "    # Create residual\n",
    "    residual_energy = sg_data.loc[:,'Production_CH'] - prod.sum(axis=1) # everything in \"Residue_other\"\n",
    "    \n",
    "    # Split residual into its nature\n",
    "    all_prod[\"Residual_Hydro_CH\"] = residual_energy * gap.loc[prod.index, \"Hydro_Res\"]\n",
    "    all_prod[\"Residual_Other_CH\"] = residual_energy * gap.loc[prod.index, \"Other_Res\"]\n",
    "    \n",
    "    # Reorder columns: residual as first production sources of Swizerland\n",
    "    cols = [\"Residual_Hydro_CH\",\"Residual_Other_CH\"] + init_cols\n",
    "    return all_prod.loc[:,cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Include Global residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_global_residual(Gen=None, freq='H', sg_data=None, prod_gap=None, is_verbose=False):\n",
    "    \"\"\"Function to add the residual swiss production\n",
    "    Parameter:\n",
    "        Gen: Gen: information about all production and cross-border flows (dict of pandas DataFrames)\n",
    "        freq: the frequence (granularity)\n",
    "        sg_data: information from SwissGrid (pandas DataFrame)\n",
    "        prod_gap: information about the nature of the residual (pandas DataFrame)\n",
    "        is_verbose: to display information\n",
    "    Return:\n",
    "        dict of modified generation and cross-border flows\n",
    "    \"\"\"\n",
    "    #######################\n",
    "    ###### Add Residue data\n",
    "    #######################\n",
    "    if is_verbose: print(\"\\t5/5 - Add Residual...\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    # Set all residual prod.\n",
    "    for f in Gen.keys():\n",
    "        if f==\"CH\":\n",
    "            # Check the availability of residual data\n",
    "            check_residual_avaliability(prod=Gen[f], residual=prod_gap, freq=freq)\n",
    "            \n",
    "            # set the two residual kinds as CH prod\n",
    "            Gen[f] = import_residual(Gen[f], sg_data=sg_data, gap=prod_gap)\n",
    "            \n",
    "        else: # for all other countries\n",
    "            Gen[f][\"Residual_Hydro_{}\".format(f)] = np.zeros((Gen[f].shape[0],1))\n",
    "            Gen[f][\"Residual_Other_{}\".format(f)] = np.zeros((Gen[f].shape[0],1))\n",
    "            Gen[f] = Gen[f][list(Gen[f].columns[-2:])+list(Gen[f].columns[:-2])] # Move empty residual\n",
    "            \n",
    "    return Gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Include local residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def include_local_residual(mix_data=None, sg_data=None, local_prod=None, gap=None, freq='H', target='CH'):\n",
    "    \"\"\"Funcion to include a local residual directly into the electric mix information.\n",
    "    Parameter:\n",
    "        mix_data: the electric mix table (pandas DataFrame)\n",
    "        sg_data: information from SwissGrid (pandas DataFrame)\n",
    "        local_prod: the production and exchanges in MWh of the target country (pandas DataFrame)\n",
    "        gap: information about the nature of the residual (pandas DataFrame)\n",
    "        freq: the time step\n",
    "        target: the target country\n",
    "    Return:\n",
    "        modified mix table\n",
    "    \"\"\"\n",
    "    # Check the availability\n",
    "    check_residual_avaliability(prod=local_prod, residual=gap, freq=freq)\n",
    "    \n",
    "    # Relative part of the residual production in the elec produced & entering for CH\n",
    "    residual = define_local_gap(local_prod=local_prod, sg_data=sg_data, freq=freq, gap=gap)\n",
    "    \n",
    "    # Adapt the mix to relative residual production\n",
    "    new_mix = adjust_mix_local(mix_data=mix_data, local_residual=residual, target=target)\n",
    "    \n",
    "    return new_mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define local gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_local_gap(local_prod, sg_data, freq='H', gap=None, target='CH'):\n",
    "    \"\"\"Function to define the relative part of residual in the electricity in the target country.\n",
    "    Returns the relative residual information.\"\"\"\n",
    "    production = [k for k in local_prod.columns if k[:3]!='Mix']\n",
    "    local_mix = [k for k in local_prod.columns if k[:3]=='Mix']\n",
    "    \n",
    "    # Residual prod in MWh\n",
    "    d = import_residual(local_prod.loc[:,production], sg_data=sg_data, gap=gap)\n",
    "\n",
    "    ## Add the mix -> Total produced + imported on the teritory\n",
    "    d = pd.concat( [d, local_prod.loc[:,local_mix]], axis=1) # set back the imports\n",
    "\n",
    "    ## Compute relative amount of residual column(s)\n",
    "    residual_col = [k for k in d.columns if k.split(\"_\")[0]==\"Residual\"]\n",
    "    for k in residual_col:\n",
    "        d.loc[:,k] /= d.sum(axis=1)\n",
    "\n",
    "    return d.loc[:,residual_col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust mix local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_mix_local(mix_data, local_residual, target='CH'):\n",
    "    \"\"\"Function to modify the mix and integrate a local residual. Returns the modified mix data.\"\"\"\n",
    "    new_mix = mix_data.copy()\n",
    "\n",
    "    ### Adjust the productions directly into electricity mix matrix\n",
    "    new_mix.loc[:,f'Mix_{target}'] -=1 # Not consider the part produced and directly consummed in Swizerland\n",
    "    for c in new_mix.columns:\n",
    "        new_mix.loc[:,c] *= (1-local_residual.sum(axis=1)) # Reduce the actual part of the kWh\n",
    "    for c in local_residual.columns: # put all the residual\n",
    "        new_mix[c] = local_residual.loc[:,c] # Add the part of Residue\n",
    "    \n",
    "    # Locate first column for producers of target country\n",
    "    lim = list(new_mix.columns).index([k for k in new_mix.columns\n",
    "                                       if ((k[:3]!=\"Mix\")&(k[-3:]==f\"_{target}\"))][0])\n",
    "    \n",
    "    # set back residual as first producer(s) of the target country\n",
    "    new_col = (list(new_mix.columns)[:lim] + list(local_residual.columns)\n",
    "               + list(new_mix.columns)[lim:-local_residual.shape[1]] )\n",
    "    new_mix = new_mix.loc[:,new_col] # Reorder the columns\n",
    "    new_mix.loc[:,f'Mix_{target}'] += 1 # Bring back the part of electricity produced and directly consummed in Swizerland\n",
    "\n",
    "    return new_mix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Saving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save impact vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_impact_vector(impact_matrix, savedir, cst_import=False, residual=False):\n",
    "    \"\"\"Function to save the impact matrix.\n",
    "    Parameter:\n",
    "        impact matrix: the table with the impact factors (pandas DataFrame)\n",
    "        savedir: the directory where to save (str)\n",
    "        cst_import: if constant exchange impacts are considered (bool, default: False)\n",
    "        residual: if a residual is considered (bool, default: False)\n",
    "    \"\"\"\n",
    "    add_on = \"\"\n",
    "    if cst_import: add_on += \"_CstImp\"\n",
    "    if residual: add_on += \"_Res\"\n",
    "    file_name = f\"Impact_Vector{add_on}.csv\"\n",
    "    \n",
    "    impact_matrix.to_csv(savedir + file_name, sep=\";\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(data, savedir, name, target, freq='H'):\n",
    "    \"\"\"Function to save the datasets with information of the frequency.\n",
    "    \n",
    "    Parameter:\n",
    "        data: the dataset (pandas DataFrame)\n",
    "        savedir: the directory where to save (str)\n",
    "        name: the name of the file (excluding extension and frequency info) (str)\n",
    "        freq: the frequency (str)\n",
    "    \"\"\"\n",
    "    ### Formating the time extension\n",
    "    tPass = {'15min':'15min','30min':'30min',\"H\":\"hour\",\"D\":\"day\",'d':'day','W':\"week\",\n",
    "             \"w\":\"week\",\"MS\":\"month\",\"M\":\"month\",\"YS\":\"year\",\"Y\":\"year\"}\n",
    "    ### Saving\n",
    "    data.to_csv(savedir+f\"{name}_{target}_{tPass[freq]}.csv\",sep=\";\",index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_frequency(freq):\n",
    "    \"\"\"Verifies if the requested frequency is supported\"\"\"\n",
    "    allowed = [\"Y\",\"YS\",\"M\",\"MS\",\"W\",\"w\",\"D\",\"d\",\"H\",\"30min\",\"15min\"]\n",
    "    if freq not in allowed:\n",
    "        raise KeyError(f'the specified timestep must be in {allowed}')\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check residual availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_residual_avaliability(prod, residual, freq='H'):\n",
    "    \"\"\"Verifies if the residual information are available for the whole duration.\n",
    "    Parameter:\n",
    "        prod: the production data where to add the residual\n",
    "        residual: the residual data to check the availability of.\n",
    "    Return:\n",
    "        True, if no exception is raised.\n",
    "    \"\"\"\n",
    "    available=True\n",
    "    text=\"\"\n",
    "    if freq!=\"Y\": # NOT yearly step of time\n",
    "        if (( (prod.index.month[0]<residual.index.month[0])\n",
    "              &(prod.index.year[0]==residual.index.year[0]))\n",
    "            |(prod.index.year[0]<residual.index.year[0])):\n",
    "            text+=\"\\nResidual data only avaliable for {}-{}. \".format(residual.index.year[0],\n",
    "                                                                      residual.index.month[0])\n",
    "            text+=\"Data from {}-{} required.\\n\".format(prod.index.year[0],prod.index.month[0])\n",
    "            available=False\n",
    "        if (( (prod.index.month[-1]>residual.index.month[-1])\n",
    "              &(prod.index.year[-1]==residual.index.year[-1]))\n",
    "            |(prod.index.year[-1]>residual.index.year[-1])):\n",
    "            text+=\"\\nResidual data only available until {}-{}. \".format(residual.index.year[-1],\n",
    "                                                                        residual.index.month[-1])\n",
    "            text+=\"Data until {}-{} required.\".format(prod.index.year[-1],prod.index.month[-1])\n",
    "            available=False\n",
    "    else: # yearly step of time\n",
    "        if (prod.index.year[0]<residual.index.year[0]):\n",
    "            text+=\"\\nResidual data only starting at {}. \".format(residual.index.year[0])\n",
    "            text+=\"Data starting at {} required.\\n\".format(prod.index.year[0])\n",
    "            available=False\n",
    "        if prod.index.year[-1]>residual.index.year[-1]:\n",
    "            text+=\"\\nResidual data only until {}. \".format(residual.index.year[-1])\n",
    "            text+=\"Data until {} required.\".format(prod.index.year[-1])\n",
    "            available=False\n",
    "            \n",
    "    if not available:\n",
    "        raise IndexError(text)\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parameter():\n",
    "    \"\"\"Parameter object adapted to the execution of the algorithm.\n",
    "    \n",
    "    Attributes:\n",
    "        - path: FilePath object containing information about path to different documents.\n",
    "        - ctry: the (sorted) list of countries to include\n",
    "        - target: the target country where to compute the mix and impact.\n",
    "        - start: starting date (utc)\n",
    "        - end: ending date (utc)\n",
    "        - freq: the time step (15min, 30min, H, d, W, M or Y)\n",
    "        - timezone: the timezone to convert in, in the end\n",
    "        - cst_imports: boolean to consider a constant impact for the imports\n",
    "        - sg_imports: boolean to replace Entso exchanges by SwissGrid exchanges\n",
    "        - net_exchanges: boolean to consider net exchanges at each border (no bidirectional)\n",
    "        - residual_local: to include a residual (for CH) as if it was all consumed in the country.\n",
    "        - residual_global: to include a residual (for CH) that can be exchanged.\n",
    "    \n",
    "    Methods:\n",
    "        - from_excel: to load parameters from a excel sheet\n",
    "        - __setattr__: to allow simple changes of parameter values.\n",
    "                    + easy use: parameter_object.attribute = new_value\n",
    "                    + start and end remain datetimes even if strings are passed\n",
    "                    + ctry remain a sorted list even if an unsorted list is passed\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.path = Filepath()\n",
    "        \n",
    "        self.ctry = sorted([\"CH\",\"FR\",\"IT\",\"DE\",\"CZ\",\"AT\"])\n",
    "        self.target = \"CH\"\n",
    "        \n",
    "        self.start = pd.to_datetime(\"2018-01-01 00:00\", yearfirst=True) # first considered date\n",
    "        self.end = pd.to_datetime(\"2018-12-31 23:00\", yearfirst=True) # last considered date\n",
    "        self.freq = 'H'\n",
    "        self.timezone = 'CET'\n",
    "        \n",
    "        self.cst_imports = False\n",
    "        self.sg_imports = False\n",
    "        self.net_exchanges = False\n",
    "        self.network_losses = False\n",
    "        self.residual_local = False\n",
    "        self.residual_global = False\n",
    "        \n",
    "    def __repr__(self):\n",
    "        text = {}\n",
    "        attributes = [\"ctry\",\"target\",\"start\",\"end\",\"freq\",\"timezone\",\"cst_imports\",\"net_exchanges\",\n",
    "                      \"network_losses\",\"sg_imports\", \"residual_local\", \"residual_global\"]\n",
    "        for a in attributes:\n",
    "            text[a] = getattr(self, a)\n",
    "        \n",
    "        return \"\\n\".join( [f\"{a} --> {text[a]}\" for a in text] ) + f\"\\n{self.path}\"\n",
    "    \n",
    "    def _dates_from_excel(self, array):\n",
    "        adapt = lambda x: (\"0\" if x<10 else \"\") + str(x)\n",
    "        date = array.fillna(0)\n",
    "        return \"{0}-{1}-{2} {3}:{4}\".format(*date.apply(adapt).values)\n",
    "    \n",
    "    def _set_to_None(self):\n",
    "        attributes = [a for a in dir(self) if ((a[0]!=\"_\")&(not callable( getattr(self, a) )))]\n",
    "        for a in attributes:\n",
    "            if np.all( pd.isna(getattr(self, a)) ): setattr( self, a, None )\n",
    "                \n",
    "    def _set_bool(self, value):\n",
    "        if pd.isna(value):\n",
    "            return None\n",
    "        else:\n",
    "            return bool(value)\n",
    "        \n",
    "    def __setattr__(self, name, value):\n",
    "        if name in ['start','end']:\n",
    "            super().__setattr__(name, pd.to_datetime(value, yearfirst=True)) # set as time\n",
    "        elif name == 'ctry':\n",
    "            super().__setattr__(name, sorted(value)) # always keep sorted\n",
    "        else:\n",
    "            super().__setattr__(name, value) # otherwise just set value\n",
    "    \n",
    "    def from_excel(self, excel):\n",
    "        param_excel = pd.read_excel(excel, sheet_name=\"Parameter\", index_col=0, header=None)\n",
    "        \n",
    "\n",
    "        self.ctry = np.sort(param_excel.loc[\"countries\"].dropna().values)\n",
    "        self.target = param_excel.loc['target'].iloc[0]\n",
    "        \n",
    "        self.start = self._dates_from_excel(param_excel.loc['start'])\n",
    "        self.end = self._dates_from_excel(param_excel.loc['end'])\n",
    "        self.freq = param_excel.loc['frequency'].iloc[0]\n",
    "        self.timezone = param_excel.loc['timezone'].iloc[0]\n",
    "\n",
    "        self.cst_imports = self._set_bool(param_excel.loc['constant exchanges'].iloc[0])\n",
    "        self.sg_imports = self._set_bool(param_excel.loc['exchanges from swissGrid'].iloc[0])\n",
    "        self.net_exchanges = self._set_bool(param_excel.loc['net exchanges'].iloc[0])\n",
    "        self.network_losses = self._set_bool(param_excel.loc['network losses'].iloc[0])\n",
    "        self.residual_local = self._set_bool(param_excel.loc['residual local'].iloc[0])\n",
    "        self.residual_global = self._set_bool(param_excel.loc['residual global'].iloc[0])\n",
    "\n",
    "        \n",
    "        self.path = self.path.from_excel(excel)\n",
    "        \n",
    "        self._set_to_None()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Filepath():\n",
    "    \"\"\"Filepath object adapted to the execution of the algorithm and the Parameter class.\n",
    "    \n",
    "    Attributes:\n",
    "        - rootdir: root directory of the experiment (highest common folder).\n",
    "                Useful mainly within the class\n",
    "        - generation: directory containing Entso generation files\n",
    "        - exchanges: directory containing Entso cross-border flow files\n",
    "        - savedir: directory where to save the results. Default: None (no saving)\n",
    "        - mapping: file with the mapping (impact per kWh produced for each production unit)\n",
    "        - neighbours: file gathering the list of neighbours of each european country\n",
    "        - gap: file with estimations of the nature of the residual\n",
    "        - swissGrid: file with production and cross-border flows from Swiss Grid\n",
    "        - networkLosses: file with estimation of the power grid losses.\n",
    "    \n",
    "    Methods:\n",
    "        - from_excel: load the attributes from a excel sheet.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.generation = None\n",
    "        self.exchanges = None\n",
    "        self.savedir = None\n",
    "        \n",
    "        self.mapping = None\n",
    "        self.neighbours = None\n",
    "        self.gap = None\n",
    "        self.swissGrid = None\n",
    "        self.networkLosses = None\n",
    "        \n",
    "    def __repr__(self):\n",
    "        attributes = [\"generation\",\"exchanges\",\"savedir\", \"mapping\",\"neighbours\",\n",
    "                      \"gap\",\"swissGrid\",\"networkLosses\"]\n",
    "        text = \"\"\n",
    "        for a in attributes:\n",
    "            text += f\"Filepath to {a} --> {getattr(self, a)}\\n\"\n",
    "        return text\n",
    "    \n",
    "    def __setattr__(self, name, value):\n",
    "        if pd.isna(value):\n",
    "            super().__setattr__(name, None) # set an empty info\n",
    "        elif os.path.isdir(r\"{}\".format(value)):\n",
    "            super().__setattr__(name, os.path.abspath(r\"{}\".format(value))+\"/\")\n",
    "        elif os.path.isfile(r\"{}\".format(value)):\n",
    "            super().__setattr__(name, os.path.abspath(r\"{}\".format(value)))\n",
    "        else:\n",
    "            raise ValueError(f'Unidentified file or directory: {os.path.abspath(value)}')\n",
    "    \n",
    "    def from_excel(self, excel):\n",
    "        param_excel = pd.read_excel(excel, sheet_name=\"Filepath\", index_col=0, header=None)\n",
    "        \n",
    "        self.generation = param_excel.loc['generation directory'].iloc[0]\n",
    "        self.exchanges = param_excel.loc['exchange directory'].iloc[0]\n",
    "        self.savedir = param_excel.loc['saving directory'].iloc[0]\n",
    "        \n",
    "        self.mapping = param_excel.loc['mapping file'].iloc[0]\n",
    "        self.neighbours = param_excel.loc['neighboring file'].iloc[0]\n",
    "        self.gap = param_excel.loc['gap file'].iloc[0]\n",
    "        self.swissGrid = param_excel.loc['file swissGrid'].iloc[0]\n",
    "        self.networkLosses = param_excel.loc['file grid losses'].iloc[0]\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EcoDynBat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Localize from UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def localize_from_utc(data, timezone='CET'):\n",
    "    \"\"\"Converts the index of a dataset in utc to another time zone\n",
    "    Parameter:\n",
    "        data: pandas DataFrame with TimeIndex as index (time supposed to be in UTC)\n",
    "        timezone: the timezone to convert in. (str, default: CET)\n",
    "                See pandas time zones for more information.\n",
    "    Return:\n",
    "        pandas DataFrame\"\"\"\n",
    "    return data.tz_localize(tz='utc').tz_convert(tz=timezone).tz_localize(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute(p=None, excel=None, is_verbose=False):\n",
    "    \"\"\"\n",
    "    Easy all-in-one execution of the algorighm, containing\n",
    "    - the import of auxiliary data\n",
    "    - the import and correction of Entso-E data (import from files)\n",
    "    - the electricity tracking\n",
    "    - the computation of the different impacts (GWP, CED, ES2013)\n",
    "    - a data shift to the right time zone (initially all is in utc)\n",
    "    - save the data into files\n",
    "    \n",
    "    Parameter:\n",
    "        p: the parameter object (from class Parameter). Default: None\n",
    "        excel: str to the excel file with parameters. Default: None\n",
    "        is_verbose: bool to display information. Default: False\n",
    "    \n",
    "    Return:\n",
    "        dict if pandas DataFrame with the impacts of 1kWh of electricity.\n",
    "    \n",
    "    \"\"\"\n",
    "    ###########################\n",
    "    ###### PARAMETERS\n",
    "    ######\n",
    "    if p is None: # Load\n",
    "        if excel is None:\n",
    "            excel = get_default_file('ExcelFile_default.xlsx')\n",
    "        p = Parameter().from_excel(excel=excel)\n",
    "    \n",
    "    if np.logical_and(p.residual_global,p.residual_local):\n",
    "        raise ValueError(\"Residual can not be both global and local.\")\n",
    "    \n",
    "    ###########################\n",
    "    ###### LOAD DATASETS\n",
    "    ######\n",
    "    if is_verbose: print(\"Load auxiliary datasets...\")\n",
    "    # Load SwissGrid -> if Residual or SG exchanges\n",
    "    if np.logical_or(np.logical_or(p.residual_global,p.residual_local), p.sg_imports):\n",
    "        sg = load_swissGrid(path_sg=p.path.swissGrid, start=p.start, end=p.end, freq=p.freq)\n",
    "    else: sg=None\n",
    "\n",
    "    # Load Country of interest -> Always\n",
    "    neighbours = load_useful_countries(path_neighbour=p.path.neighbours, ctry=p.ctry)\n",
    "\n",
    "    # Load network losses -> if Network Loss asked\n",
    "    if p.network_losses:\n",
    "        network_losses = load_grid_losses(network_loss_path=p.path.networkLosses, start=p.start, end=p.end)\n",
    "    else: network_losses = None\n",
    "        \n",
    "    # Load production gap data -> if Residual\n",
    "    if np.logical_or(p.residual_global,p.residual_local):\n",
    "        prod_gap = load_gap_content(path_gap=p.path.gap, start=p.start, end=p.end, freq=p.freq, header=59)\n",
    "    else: prod_gap=None\n",
    "\n",
    "    # Load impact matrix\n",
    "    impact_matrix = extract_impacts(ctry=p.ctry, mapping_path=p.path.mapping, cst_import=p.cst_imports,\n",
    "                                    residual=np.logical_or(p.residual_global, p.residual_local),\n",
    "                                    target=p.target, is_verbose=True)\n",
    "    \n",
    "\n",
    "    # Load generation and exchange data from entso-e    \n",
    "    raw_prodExch = import_data(ctry=p.ctry, start=p.start, end=p.end, freq=p.freq, target=p.target,\n",
    "                               involved_countries=neighbours, prod_gap=prod_gap, sg_data=sg,\n",
    "                               path_gen=p.path.generation, path_imp=p.path.exchanges,\n",
    "                               residual_global=p.residual_global, correct_imp=p.sg_imports,\n",
    "                               is_verbose=is_verbose)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ########################\n",
    "    ###### COMPUTE TRACKING\n",
    "    ######\n",
    "    mix = track_mix(raw_data=raw_prodExch, freq=p.freq, network_losses=network_losses,\n",
    "                    target=p.target, residual_global=p.residual_global,\n",
    "                    net_exchange=p.net_exchanges, is_verbose=is_verbose)\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    ####### ADD LOCAL RESIDUAL\n",
    "    #######\n",
    "    if p.residual_local:\n",
    "        if is_verbose: print(\"Compute local residual...\")\n",
    "        local = [k for k in raw_prodExch.columns if k[-3:]==f'_{p.target}']\n",
    "        mix = include_local_residual(mix_data=mix, sg_data=sg, local_prod=raw_prodExch.loc[:,local],\n",
    "                                      gap=prod_gap, freq=p.freq)\n",
    "\n",
    "\n",
    "    ############################\n",
    "    ###### COMPUTE ELEC IMPACTS\n",
    "    ######\n",
    "    imp = compute_impacts(mix_data=mix, impact_data=impact_matrix, freq=p.freq, is_verbose=is_verbose)\n",
    "    \n",
    "    \n",
    "    ###############################\n",
    "    ###### TRANSLATE INTO TIMEZONE\n",
    "    ######\n",
    "    if p.timezone is not None:\n",
    "        if is_verbose: print(f\"Adapt timezone: UTC >> {p.timezone}\")\n",
    "        raw_prodExch = localize_from_utc(data=raw_prodExch, timezone=p.timezone)\n",
    "        mix = localize_from_utc(data=mix, timezone=p.timezone)\n",
    "        for k in imp:\n",
    "            imp[k] = localize_from_utc(data=imp[k], timezone=p.timezone)\n",
    "    \n",
    "    \n",
    "    ################################\n",
    "    ####### SAVE DATA\n",
    "    #######\n",
    "    if p.path.savedir is not None:\n",
    "        if is_verbose: print(\"Save data...\")\n",
    "        save_impact_vector(impact_matrix, savedir=p.path.savedir, cst_import=p.cst_imports,\n",
    "                           residual=np.logical_or(p.residual_global,p.residual_local))\n",
    "        save_dataset(data=raw_prodExch, savedir=p.path.savedir, name=\"ProdExchange\", freq=p.freq)\n",
    "        save_dataset(data=mix, savedir=p.path.savedir, name=f\"Mix\", target=p.target, freq=p.freq)\n",
    "        for k in imp:\n",
    "            save_dataset(data=imp[k], savedir=p.path.savedir, name=f'Impact_{k.replace(\"_\",\"-\")}',\n",
    "                         target=p.target, freq=p.freq)\n",
    "    \n",
    "    if is_verbose: print(\"done.\")\n",
    "    return imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Commentaires**:\n",
    "+ Il faut trouver un moyer d'importer sa propre donnée d'Entso-E (hors utilisation de fichiers). Dépendra du format de données renvoyé par l'ancien fichier R une fois traduit en Python. Les fonctions impliquées seront alors **import data**, **import generation** et **import exchanges**. La méthode à entreprendre devra être similaire à l'esprit de la fonction *Load mix*.\n",
    "+ Le paramètre *correct_imp* et la fonction *adjust_exchanges* consistent à mettre les échanges de swiss-grid à la place ce ceux d'Entso-E. C'est prévu. Il faut faire une litérature pour s'en rapeler...\n",
    "+ Dans *load mix*, si un dataset est donnée en entrée, le pas de temps n'est pas adapté. Il est supposé que le resampling a déjà eu lieux.\n",
    "+ Les fonctions d'import sont spécifiques aux formats de donnée utilisées durant le projet EcoDynBat. Chaque brique est facilement modifiable. De fait, il faut changer ces briques pour s'adapter à des formats de donnée plus communs.\n",
    "+ ***ATTENTION !!*** Les fonction **import generation** et **import exchanges** ont une data (2021) spécifiée pour le choix des fichiers. Cela doit être enlevé à la fin, lorsque tous les fichiers des dossiers seront de la même date.\n",
    "+ ***ATTENTION !!*** Les fonctions **import generation** et **import exchanges** comportent toujours un resampling **heure** $\\to$ **15 minutes**, comme les donneés disponibles dans les fichiers sont en heure. Il faudra supprimer les lignes dès que les fichiers seront en 15 minutes. **Le vrai resampling** (si le pas de temps désiré n'est pas 15 minutes) est présent dans la fonction *resample generation* pour les données de production et directement à la fin de *import exchanges* pour les échanges. **Les données swissGrid** sont remises au bon pas de temps dès l'import de données dans la fonction *load swissGrid*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stupid_check(array1, array2):\n",
    "    \"\"\"Function to compare the content of two arrays.\n",
    "    Useful for debuging.\n",
    "    \"\"\"\n",
    "    print(\"In (1), not in (2): \")\n",
    "    for k in array1:\n",
    "        if k not in array2:\n",
    "            print(k)\n",
    "\n",
    "    print(\"\\nIn (2), not in (1): \")\n",
    "    for k in array2:\n",
    "        if k not in array1:\n",
    "            print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_country(ctry, data):\n",
    "    \"\"\"Function to group datasets per country\"\"\"\n",
    "    per_country = []\n",
    "    for c in ctry:\n",
    "        cols = [k for k in data.columns if k[-3:]==f\"_{c}\"]\n",
    "        per_country.append(pd.Series(data.loc[:,cols].sum(axis=1), name=c))\n",
    "\n",
    "    return pd.DataFrame(per_country).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_per_unit(data):\n",
    "    \"\"\"Function to group datasets per type of unit, regardless of the country of origin\"\"\"\n",
    "    unit_list = np.unique([k[:-3] for k in data.columns]) # List the different production units\n",
    "    \n",
    "    per_unit = []\n",
    "    for u in unit_list:\n",
    "        cols = [k for k in data.columns if k[:-3]==u] # collect the useful columns\n",
    "        per_unit.append(pd.Series(data.loc[:,cols].sum(axis=1), name=u)) # aggregate\n",
    "\n",
    "    return pd.DataFrame(per_unit).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ctry --> ['AT', 'CH', 'CZ', 'DE', 'FR', 'IT']\n",
       "target --> CH\n",
       "start --> 2016-01-01 00:00:00\n",
       "end --> 2016-01-03 23:45:00\n",
       "freq --> 15min\n",
       "timezone --> CET\n",
       "cst_imports --> False\n",
       "net_exchanges --> False\n",
       "network_losses --> True\n",
       "sg_imports --> True\n",
       "residual_local --> True\n",
       "residual_global --> False\n",
       "Filepath to generation --> /home/francois/Documents/EcoDynBat/Calculs_EcoDynBat/Generation_Data/\n",
       "Filepath to exchanges --> /home/francois/Documents/EcoDynBat/Calculs_EcoDynBat/Importation_Data/\n",
       "Filepath to savedir --> None\n",
       "Filepath to mapping --> /home/francois/Documents/EcoDynBat/Calculs_EcoDynBat/Mappings/Mapping_case_residue_mean.xlsx\n",
       "Filepath to neighbours --> None\n",
       "Filepath to gap --> None\n",
       "Filepath to swissGrid --> None\n",
       "Filepath to networkLosses --> None"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Define the parameters for the execution\n",
    "\n",
    "p = Parameter().from_excel(excel=\"./EcoDynBat_tests.xlsx\")\n",
    "p.start= '2016-01-01 00:00'\n",
    "p.end= '2016-01-03 23:45'\n",
    "\n",
    "p.target = 'CH'\n",
    "\n",
    "p.freq = \"15min\"\n",
    "p.residual_global = False\n",
    "p.residual_local = True\n",
    "p.net_exchanges = False\n",
    "p.network_losses = True\n",
    "p.sg_imports = True\n",
    "p.cst_imports = False\n",
    "\n",
    "#p.path.savedir = p.path.rootdir+\"EcoDynBat2/local_output/\"\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load auxiliary datasets...\n",
      "Extraction of impact vector...\n",
      "\t. Mix_Other / AT / CH / CZ / DE / FR / IT + Residual .\n",
      "Load generation data...\n",
      "Correction of generation data:\n",
      "\t1/4 - Gather missing values...\n",
      "\t2/4 - Sort missing values...\n",
      "\t3/4 - Fill missing values...\n",
      "Get and reduce importation data...\n",
      "\t1/6 - AT...\n",
      "\t2/6 - CH...\n",
      "\t3/6 - CZ...\n",
      "\t4/6 - DE...\n",
      "\t5/6 - FR...\n",
      "\t6/6 - IT...\n",
      "Adapt Exchage Data of Swizerland...\n",
      "Gather generation and importation...\n",
      "Import of data: 3.2 sec\n",
      "Importing information...\n",
      "Tracking origin of electricity...\n",
      "\tcompute for day 3/3   \n",
      "\tElectricity tracking: 3.1 sec.\n",
      "\n",
      "Compute local residual...\n",
      "Compute the electricity impacts...\n",
      "\tGlobal...\n",
      "\tGWP...\n",
      "\tCED_renewable...\n",
      "\tCED_non-renewable...\n",
      "\tES2013...\n",
      "Impact computation: 0.8 sec.\n",
      "Adapt timezone: UTC >> CET\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Execute the whole experiment\n",
    "imp = execute(p=p, excel=None, is_verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents the impact of 1kWh of electricity consumed in Swizerland\n",
    "imp['Global'].plot(figsize=(15,6),grid=True, subplots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents the detail per country of origin of that kWh of electricity\n",
    "per_country = compute_per_country(p.ctry, imp['GWP'][[k for k in imp['GWP'].columns if k[:3]!='Mix']])\n",
    "per_country.plot.area(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents the detail per country of origin of that kWh of electricity\n",
    "per_unit = compute_per_unit(imp['GWP'][[k for k in imp['GWP'].columns if k[:3]!='Mix']])\n",
    "per_unit.plot.area(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represents the detail per country of origin of that kWh of electricity\n",
    "per_unit = compute_per_unit(imp['GWP'][[k for k in imp['GWP'].columns if k.find('Residual')!=-1]])\n",
    "per_unit.plot.area(figsize=(15,5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EcoDyn",
   "language": "python",
   "name": "env_ecodyn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
